import os
import sys
import datetime
import json
import re
from pathlib import Path
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

import uvicorn
import requests
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from bs4 import BeautifulSoup

# gemini_clientã‚’é©åˆ‡ãªãƒ‘ã‚¹ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from nook.local.common.gemini_client import create_client

app = FastAPI()

# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
data_dir = os.environ.get("DATA_DIR", "./data")
templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "templates")

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
templates = Jinja2Templates(directory=templates_dir)

# staticãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
static_dir = os.path.join(templates_dir, "static")
os.makedirs(static_dir, exist_ok=True)

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«ã®æä¾›
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# å¯¾è±¡ã®ã‚¢ãƒ—ãƒªåãƒªã‚¹ãƒˆ
app_names = [
    "github_trending",
    "hacker_news",
    "paper_summarizer",
    "reddit_explorer",
    "tech_feed",
]

# å¤©æ°—ã‚¢ã‚¤ã‚³ãƒ³ã®å¯¾å¿œè¡¨ï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ï¼‰
WEATHER_ICONS = {
    "100": "â˜€ï¸",  # æ™´ã‚Œ
    "101": "ğŸŒ¤ï¸",  # æ™´ã‚Œæ™‚ã€…ãã‚‚ã‚Š
    "200": "â˜ï¸",  # ãã‚‚ã‚Š
    "201": "â›…",  # ãã‚‚ã‚Šæ™‚ã€…æ™´ã‚Œ
    "202": "ğŸŒ§ï¸",  # ãã‚‚ã‚Šä¸€æ™‚é›¨
    "300": "ğŸŒ§ï¸",  # é›¨
    "301": "ğŸŒ¦ï¸",  # é›¨æ™‚ã€…æ™´ã‚Œ
    "400": "ğŸŒ¨ï¸",  # é›ª
}

def get_weather_data():
    """
    æ°—è±¡åºã®APIã‹ã‚‰æ±äº¬ã®å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹
    """
    try:
        response = requests.get(
            "https://www.jma.go.jp/bosai/forecast/data/forecast/130000.json", timeout=5
        )
        response.raise_for_status()
        data = response.json()

        # æ±äº¬åœ°æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        tokyo = next(
            (
                area
                for area in data[0]["timeSeries"][2]["areas"]
                if area["area"]["name"] == "æ±äº¬"
            ),
            None,
        )
        tokyo_weather = next(
            (
                area
                for area in data[0]["timeSeries"][0]["areas"]
                if area["area"]["code"] == "130010"
            ),
            None,
        )

        if tokyo and tokyo_weather:
            # ç¾åœ¨ã®æ°—æ¸©ï¼ˆtemps[0]ãŒæœ€ä½æ°—æ¸©ã€temps[1]ãŒæœ€é«˜æ°—æ¸©ï¼‰
            temps = tokyo["temps"]
            weather_code = tokyo_weather["weatherCodes"][0]
            weather_icon = WEATHER_ICONS.get(weather_code, "")

            return {
                "temp": temps[0],
                "weather_code": weather_code,
                "weather_icon": weather_icon,
            }
    except Exception as e:
        print(f"Error fetching weather data: {e}")

    # ã‚¨ãƒ©ãƒ¼æ™‚ã‚„ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
    return {
        "temp": "--",
        "weather_code": "100",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ™´ã‚Œ
        "weather_icon": WEATHER_ICONS.get("100", "â˜€ï¸"),
    }

def extract_links(text: str) -> list[str]:
    """Markdownãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹"""
    # Markdownå½¢å¼ã®ãƒªãƒ³ã‚¯ [text](url) ã‚’æŠ½å‡º
    markdown_links = re.findall(r"\[([^\]]+)\]\(([^)]+)\)", text)
    # ã‚‚ã—[text]ã®éƒ¨åˆ†ãŒ[Image]ã¾ãŸã¯[Video]ã®å ´åˆã¯ã€ãã®éƒ¨åˆ†ã‚’é™¤å¤–
    markdown_links = [
        (text, url)
        for text, url in markdown_links
        if not text.startswith("[Image]") and not text.startswith("[Video]")
    ]
    # é€šå¸¸ã®URLã‚‚æŠ½å‡º
    urls = re.findall(r"(?<![\(\[])(https?://[^\s\)]+)", text)

    return [url for _, url in markdown_links] + urls

def fetch_url_content(url: str) -> str | None:
    """URLã®å†…å®¹ã‚’å–å¾—ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã‚’å‰Šé™¤
        for element in soup(["script", "style", "nav", "header", "footer"]):
            element.decompose()

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆarticle, main, ã¾ãŸã¯æœ¬æ–‡è¦ç´ ï¼‰
        main_content = soup.find("article") or soup.find("main") or soup.find("body")
        if main_content:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            text = " ".join(main_content.get_text(separator=" ").split())
            # é•·ã™ãã‚‹å ´åˆã¯æœ€åˆã®1000æ–‡å­—ã«åˆ¶é™
            return text[:1000] + "..." if len(text) > 1000 else text

        return None
    except Exception as e:
        print(f"Error fetching URL {url}: {e}")
        return None

def fetch_markdown(app_name: str, date_str: str) -> str:
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¢ãƒ—ãƒªåã¨æ—¥ä»˜ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ä¸Šã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    """
    file_path = os.path.join(data_dir, f"{app_name}/{date_str}.md")
    try:
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        else:
            return f"No data available for {app_name} on {date_str}"
    except Exception as e:
        return f"Error reading {file_path}: {e}"

@app.get("/", response_class=HTMLResponse)
async def index(request: Request, date: str = None):
    if date is None:
        date = datetime.date.today().strftime("%Y-%m-%d")
    
    # åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
    available_dates = []
    for app_name in app_names:
        app_dir = os.path.join(data_dir, app_name)
        if os.path.exists(app_dir):
            for file_name in os.listdir(app_dir):
                if file_name.endswith(".md"):
                    date_str = file_name.replace(".md", "")
                    if date_str not in available_dates:
                        available_dates.append(date_str)
    
    available_dates.sort(reverse=True)
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
    contents = {name: fetch_markdown(name, date) for name in app_names}
    
    # å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    weather_data = get_weather_data()

    # å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’æ–°ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
    weather = {
        "temperature": weather_data["temp"],
        "icon": "sunny"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    }
    
    # å¤©æ°—ã‚³ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’è¨­å®š
    weather_code = weather_data.get("weather_code", "100")
    if weather_code.startswith("1"):  # æ™´ã‚Œç³»
        weather["icon"] = "sunny"
    elif weather_code.startswith("2"):  # ãã‚‚ã‚Šç³»
        weather["icon"] = "cloudy"
    elif weather_code.startswith("3"):  # é›¨ç³»
        weather["icon"] = "rainy"
    elif weather_code.startswith("4"):  # é›ªç³»
        weather["icon"] = "snowy"

    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "contents": contents,
            "selected_date": date,
            "app_names": app_names,
            "weather": weather,
            "available_dates": available_dates,
        },
    )

@app.get("/fetch_markdown", response_class=JSONResponse)
async def get_markdown(app_name: str, date: str):
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¢ãƒ—ãƒªåã¨æ—¥ä»˜ã®Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    content = fetch_markdown(app_name, date)
    return {"content": content}

@app.get("/api/weather", response_class=JSONResponse)
async def get_weather():
    """å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return get_weather_data()

_MESSAGE = """
ä»¥ä¸‹ã®è¨˜äº‹ã«é–¢é€£ã—ã¦ã€æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ç”¨ã„ã¦äº‹å®Ÿã‚’ç¢ºèªã—ãªãŒã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã¦ã§ãã‚‹ã ã‘è©³ç´°ã«ç­”ãˆã¦ãã ã•ã„ã€‚
ãªãŠã€å›ç­”ã¯Markdownå½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

[è¨˜äº‹]

{markdown}

{additional_context}

[ãƒãƒ£ãƒƒãƒˆå±¥æ­´]

'''
{chat_history}
'''

[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æ–°ã—ã„è³ªå•]

'''
{message}
'''

ãã‚Œã§ã¯ã€å›ç­”ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""

@app.post("/chat/{topic_id}")
async def chat(topic_id: str, request: Request):
    data = await request.json()
    message = data.get("message")
    markdown = data.get("markdown")
    chat_history = data.get("chat_history", "ãªã—")  # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å—ã‘å–ã‚‹

    # markdownã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    links = extract_links(markdown) + extract_links(message)

    # ãƒªãƒ³ã‚¯ã®å†…å®¹ã‚’å–å¾—
    additional_context = []
    for url in links:
        if content := fetch_url_content(url):
            additional_context.append(f"- Content from {url}:\n\n'''{content}'''\n\n")

    # è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã€markdownã«è¿½åŠ 
    if additional_context:
        additional_context = (
            "\n\n[è¨˜äº‹ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å«ã¾ã‚Œã‚‹ãƒªãƒ³ã‚¯ã®å†…å®¹](ã†ã¾ãå–å¾—ã§ãã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™)\n\n"
            + "\n\n".join(additional_context)
        )
    else:
        additional_context = ""

    formatted_message = _MESSAGE.format(
        markdown=markdown,
        additional_context=additional_context,
        chat_history=chat_history,
        message=message,
    )

    gemini_client = create_client(use_search=True)
    response_text = gemini_client.chat_with_search(formatted_message)

    return {"response": response_text}

if __name__ == "__main__":
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    for app_name in app_names:
        os.makedirs(os.path.join(data_dir, app_name), exist_ok=True)
    
    print(f"Starting Nook viewer at http://localhost:8080")
    uvicorn.run(app, host="0.0.0.0", port=8080)